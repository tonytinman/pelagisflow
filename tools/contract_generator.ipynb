{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Contract Generator\n",
    "\n",
    "**Purpose:** Generate ODCS v3.0.2 aligned data contracts for Nova Framework transformations.\n",
    "\n",
    "This notebook is a standalone tool for ad-hoc contract generation. It is **not** integrated into the main framework - use it independently when you need to create a new data contract for a transformation.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Configure the settings** in the Configuration cell below\n",
    "2. **Run all cells** to generate your contract\n",
    "3. **Review the output** and make any manual adjustments\n",
    "4. **Save the contract** to your desired location\n",
    "\n",
    "## What This Generates\n",
    "\n",
    "- Complete YAML data contract with all required sections\n",
    "- Schema definition based on your output columns\n",
    "- Transformation configuration linking to your Python/SQL code\n",
    "- Basic data quality rules\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**Edit the values below to configure your data contract.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# CONTRACT CONFIGURATION - Edit these values\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Contract Identity\n",
    "CONTRACT_NAME = \"my_transformation\"  # Name of the contract (no spaces, use underscores)\n",
    "CONTRACT_VERSION = \"1.0.0\"           # Semantic version\n",
    "\n",
    "# Domain & Data Product\n",
    "DOMAIN = \"sales\"                      # Business domain (e.g., sales, marketing, finance)\n",
    "DATA_PRODUCT = \"analytics\"            # Data product name\n",
    "\n",
    "# Team Information\n",
    "TEAM = {\n",
    "    \"dataOwner\": \"data-owner@example.com\",\n",
    "    \"seniorManager\": \"senior-manager@example.com\",\n",
    "    \"dataSteward\": \"data-steward@example.com\"\n",
    "}\n",
    "\n",
    "# Output Schema Configuration\n",
    "OUTPUT_CATALOG = \"gold\"               # Target catalog (gold, silver)\n",
    "OUTPUT_SCHEMA = \"sales\"               # Target schema\n",
    "OUTPUT_TABLE = \"my_transformation\"    # Target table name\n",
    "OUTPUT_FORMAT = \"delta\"               # Output format (delta, parquet)\n",
    "SCHEMA_DESCRIPTION = \"Description of what this data represents\"\n",
    "\n",
    "# Transformation Type: \"python\", \"sql\", or \"scala\"\n",
    "TRANSFORMATION_TYPE = \"python\"\n",
    "\n",
    "# Python Transformation Settings (if TRANSFORMATION_TYPE == \"python\")\n",
    "TRANSFORMATION_MODULE = \"my_transformation\"      # Python module name (without .py)\n",
    "TRANSFORMATION_CLASS = \"MyTransformation\"        # Class name (if class-based)\n",
    "TRANSFORMATION_FUNCTION = None                   # Function name (if function-based, set CLASS to None)\n",
    "\n",
    "# SQL Transformation (if TRANSFORMATION_TYPE == \"sql\")\n",
    "# Set TRANSFORMATION_SQL to your SQL query or leave as None for Python/Scala\n",
    "TRANSFORMATION_SQL = None\n",
    "\n",
    "# Pipeline Configuration\n",
    "PIPELINE_TYPE = \"transformation\"      # transformation, ingestion\n",
    "WRITE_STRATEGY = \"overwrite\"          # overwrite, append, merge, scd2\n",
    "SOFT_DELETE = False                   # Enable soft delete\n",
    "\n",
    "# Transformation Config (passed to your transformation as **kwargs)\n",
    "TRANSFORMATION_CONFIG = {\n",
    "    \"source_catalog\": \"bronze\",\n",
    "    # Add your custom config parameters here\n",
    "    # \"lookback_days\": 30,\n",
    "    # \"min_amount\": 10.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Output Schema\n",
    "\n",
    "Define the columns that your transformation will produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# OUTPUT SCHEMA DEFINITION\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# Define each column your transformation produces.\n",
    "#\n",
    "# Supported types:\n",
    "#   string, int, integer, bigint, double, float, decimal(p,s),\n",
    "#   date, timestamp, boolean, array<type>, map<key,value>, struct<...>\n",
    "#\n",
    "# Properties:\n",
    "#   name         - Column name (required)\n",
    "#   type         - Data type (required)\n",
    "#   description  - Column description (recommended)\n",
    "#   isPrimaryKey - True if part of natural key (optional)\n",
    "#   isNullable   - True if nullable, False otherwise (default: True)\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "OUTPUT_COLUMNS = [\n",
    "    {\n",
    "        \"name\": \"id\",\n",
    "        \"type\": \"bigint\",\n",
    "        \"description\": \"Unique identifier\",\n",
    "        \"isPrimaryKey\": True,\n",
    "        \"isNullable\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"name\",\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Name field\",\n",
    "        \"isNullable\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"amount\",\n",
    "        \"type\": \"decimal(18,2)\",\n",
    "        \"description\": \"Amount value\",\n",
    "        \"isNullable\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"created_date\",\n",
    "        \"type\": \"date\",\n",
    "        \"description\": \"Date of creation\",\n",
    "        \"isNullable\": True\n",
    "    },\n",
    "    # Add more columns as needed...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Quality Rules (Optional)\n",
    "\n",
    "Add validation rules for your output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# DATA QUALITY RULES (Optional)\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# Define validation rules for your output data.\n",
    "#\n",
    "# Supported rule types:\n",
    "#   not_null       - Column must not be null\n",
    "#   not_blank      - Column must not be empty string\n",
    "#   unique         - Column values must be unique\n",
    "#   min            - Minimum value (use 'value' parameter)\n",
    "#   max            - Maximum value (use 'value' parameter)\n",
    "#   between        - Value between range (use 'min' and 'max' parameters)\n",
    "#   allowed_values - Column must be one of specified values (use 'values' list)\n",
    "#   regex          - Column must match regex pattern (use 'pattern' parameter)\n",
    "#   min_length     - Minimum string length (use 'value' parameter)\n",
    "#   max_length     - Maximum string length (use 'value' parameter)\n",
    "#\n",
    "# Severity levels: error, warning, info\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "QUALITY_VALIDATION_RULES = [\n",
    "    {\n",
    "        \"column\": \"id\",\n",
    "        \"rule\": \"unique\",\n",
    "        \"severity\": \"error\"\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"id\",\n",
    "        \"rule\": \"not_null\",\n",
    "        \"severity\": \"error\"\n",
    "    },\n",
    "    {\n",
    "        \"column\": \"amount\",\n",
    "        \"rule\": \"min\",\n",
    "        \"value\": 0,\n",
    "        \"severity\": \"error\"\n",
    "    },\n",
    "    # Add more validation rules as needed...\n",
    "]\n",
    "\n",
    "# Cleansing rules (applied before validation)\n",
    "QUALITY_CLEANSING_RULES = [\n",
    "    # Example: Trim all string fields\n",
    "    # {\n",
    "    #     \"type\": \"transformation\",\n",
    "    #     \"rule\": \"trim_string_fields\"\n",
    "    # },\n",
    "    # Example: Uppercase specific columns\n",
    "    # {\n",
    "    #     \"type\": \"transformation\",\n",
    "    #     \"rule\": \"uppercase\",\n",
    "    #     \"columns\": [\"status\"]\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contract Generator Functions\n",
    "\n",
    "Run this cell to load the generator functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# CONTRACT GENERATOR - Core Functions\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "import yaml\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generate_contract(\n",
    "    name: str,\n",
    "    version: str,\n",
    "    domain: str,\n",
    "    data_product: str,\n",
    "    team: Dict[str, str],\n",
    "    schema_config: Dict[str, Any],\n",
    "    columns: List[Dict[str, Any]],\n",
    "    transformation_type: str,\n",
    "    transformation_config: Dict[str, Any],\n",
    "    pipeline_config: Dict[str, Any],\n",
    "    quality_validation: List[Dict[str, Any]] = None,\n",
    "    quality_cleansing: List[Dict[str, Any]] = None,\n",
    "    transformation_module: str = None,\n",
    "    transformation_class: str = None,\n",
    "    transformation_function: str = None,\n",
    "    transformation_sql: str = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a complete ODCS v3.0.2 data contract YAML.\n",
    "    \n",
    "    Returns:\n",
    "        YAML string of the complete data contract\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the contract structure\n",
    "    contract = {\n",
    "        \"apiVersion\": \"v3.0.2\",\n",
    "        \"kind\": \"DataContract\",\n",
    "        \"name\": name,\n",
    "        \"version\": version,\n",
    "        \"domain\": domain,\n",
    "        \"dataProduct\": data_product,\n",
    "        \"team\": team,\n",
    "    }\n",
    "    \n",
    "    # Build schema section\n",
    "    contract[\"schema\"] = {\n",
    "        \"name\": f\"{schema_config['catalog']}_{schema_config['schema']}.{schema_config['table']}\",\n",
    "        \"table\": schema_config['table'],\n",
    "        \"format\": schema_config['format'],\n",
    "        \"description\": schema_config.get('description', ''),\n",
    "        \"properties\": columns\n",
    "    }\n",
    "    \n",
    "    # Build customProperties section\n",
    "    custom_properties = {\n",
    "        \"pipelineType\": pipeline_config.get('type', 'transformation'),\n",
    "        \"writeStrategy\": pipeline_config.get('write_strategy', 'overwrite'),\n",
    "        \"softDelete\": pipeline_config.get('soft_delete', False),\n",
    "        \"transformationType\": transformation_type,\n",
    "    }\n",
    "    \n",
    "    # Add transformation-specific config\n",
    "    if transformation_type == \"python\":\n",
    "        if transformation_module:\n",
    "            custom_properties[\"transformationModule\"] = transformation_module\n",
    "        if transformation_class:\n",
    "            custom_properties[\"transformationClass\"] = transformation_class\n",
    "        if transformation_function:\n",
    "            custom_properties[\"transformationFunction\"] = transformation_function\n",
    "    elif transformation_type == \"sql\" and transformation_sql:\n",
    "        custom_properties[\"transformationSql\"] = transformation_sql\n",
    "    elif transformation_type == \"scala\":\n",
    "        if transformation_module:\n",
    "            custom_properties[\"transformationModule\"] = transformation_module\n",
    "        if transformation_class:\n",
    "            custom_properties[\"transformationClass\"] = transformation_class\n",
    "    \n",
    "    # Add transformation config if provided\n",
    "    if transformation_config:\n",
    "        custom_properties[\"transformationConfig\"] = transformation_config\n",
    "    \n",
    "    contract[\"customProperties\"] = custom_properties\n",
    "    \n",
    "    # Build quality section\n",
    "    quality_rules = []\n",
    "    \n",
    "    if quality_cleansing:\n",
    "        quality_rules.extend(quality_cleansing)\n",
    "    \n",
    "    if quality_validation:\n",
    "        quality_rules.extend(quality_validation)\n",
    "    \n",
    "    if quality_rules:\n",
    "        contract[\"quality\"] = {\n",
    "            \"validation\": quality_validation or []\n",
    "        }\n",
    "    \n",
    "    return contract\n",
    "\n",
    "\n",
    "def contract_to_yaml(contract: Dict[str, Any], add_header: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Convert contract dictionary to formatted YAML string.\n",
    "    \n",
    "    Args:\n",
    "        contract: Contract dictionary\n",
    "        add_header: Whether to add documentation header\n",
    "        \n",
    "    Returns:\n",
    "        Formatted YAML string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Custom YAML representer to handle multiline strings properly\n",
    "    def str_representer(dumper, data):\n",
    "        if '\\n' in data:\n",
    "            return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "    \n",
    "    yaml.add_representer(str, str_representer)\n",
    "    \n",
    "    # Generate YAML\n",
    "    yaml_content = yaml.dump(\n",
    "        contract,\n",
    "        default_flow_style=False,\n",
    "        sort_keys=False,\n",
    "        allow_unicode=True,\n",
    "        width=120\n",
    "    )\n",
    "    \n",
    "    if add_header:\n",
    "        header = f\"\"\"# ════════════════════════════════════════════════════════════════════════════\n",
    "# Data Contract: {contract.get('name', 'unnamed')}\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "# API Version: ODCS v3.0.2\n",
    "#\n",
    "# This contract was generated using the Contract Generator tool.\n",
    "# Review and customize as needed before deploying.\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "        return header + yaml_content\n",
    "    \n",
    "    return yaml_content\n",
    "\n",
    "\n",
    "def save_contract(yaml_content: str, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save contract YAML to file.\n",
    "    \n",
    "    Args:\n",
    "        yaml_content: YAML string to save\n",
    "        file_path: Path to save the file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "    print(f\"Contract saved to: {file_path}\")\n",
    "\n",
    "\n",
    "print(\"Contract generator functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Contract\n",
    "\n",
    "Run this cell to generate your data contract based on the configuration above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# GENERATE THE CONTRACT\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Build schema config\n",
    "schema_config = {\n",
    "    \"catalog\": OUTPUT_CATALOG,\n",
    "    \"schema\": OUTPUT_SCHEMA,\n",
    "    \"table\": OUTPUT_TABLE,\n",
    "    \"format\": OUTPUT_FORMAT,\n",
    "    \"description\": SCHEMA_DESCRIPTION\n",
    "}\n",
    "\n",
    "# Build pipeline config\n",
    "pipeline_config = {\n",
    "    \"type\": PIPELINE_TYPE,\n",
    "    \"write_strategy\": WRITE_STRATEGY,\n",
    "    \"soft_delete\": SOFT_DELETE\n",
    "}\n",
    "\n",
    "# Generate contract\n",
    "contract = generate_contract(\n",
    "    name=CONTRACT_NAME,\n",
    "    version=CONTRACT_VERSION,\n",
    "    domain=DOMAIN,\n",
    "    data_product=DATA_PRODUCT,\n",
    "    team=TEAM,\n",
    "    schema_config=schema_config,\n",
    "    columns=OUTPUT_COLUMNS,\n",
    "    transformation_type=TRANSFORMATION_TYPE,\n",
    "    transformation_config=TRANSFORMATION_CONFIG,\n",
    "    pipeline_config=pipeline_config,\n",
    "    quality_validation=QUALITY_VALIDATION_RULES,\n",
    "    quality_cleansing=QUALITY_CLEANSING_RULES,\n",
    "    transformation_module=TRANSFORMATION_MODULE,\n",
    "    transformation_class=TRANSFORMATION_CLASS,\n",
    "    transformation_function=TRANSFORMATION_FUNCTION,\n",
    "    transformation_sql=TRANSFORMATION_SQL,\n",
    ")\n",
    "\n",
    "# Convert to YAML\n",
    "contract_yaml = contract_to_yaml(contract)\n",
    "\n",
    "# Display the generated contract\n",
    "print(\"Generated Contract:\")\n",
    "print(\"=\" * 80)\n",
    "print(contract_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Contract (Optional)\n",
    "\n",
    "Uncomment and run this cell to save the contract to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# SAVE THE CONTRACT TO FILE\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Uncomment the lines below to save the contract\n",
    "\n",
    "# Option 1: Save to samples folder\n",
    "# save_contract(contract_yaml, f\"../samples/transformation_examples/{CONTRACT_NAME}.yaml\")\n",
    "\n",
    "# Option 2: Save to Databricks Volumes (update path as needed)\n",
    "# save_contract(contract_yaml, f\"/Volumes/your_catalog/nova_framework/data_contracts/{CONTRACT_NAME}.yaml\")\n",
    "\n",
    "# Option 3: Save to current directory\n",
    "# save_contract(contract_yaml, f\"{CONTRACT_NAME}.yaml\")\n",
    "\n",
    "print(\"To save the contract, uncomment one of the save_contract() lines above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Extract Schema from Existing DataFrame (Databricks)\n",
    "\n",
    "If you have an existing DataFrame, you can extract its schema to populate the OUTPUT_COLUMNS list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# HELPER: Extract schema from DataFrame\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# Run this in Databricks if you have an existing DataFrame and want to\n",
    "# generate the OUTPUT_COLUMNS list automatically.\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def extract_columns_from_dataframe(df) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract column definitions from a PySpark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        List of column definitions for OUTPUT_COLUMNS\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    \n",
    "    for field in df.schema.fields:\n",
    "        col_def = {\n",
    "            \"name\": field.name,\n",
    "            \"type\": str(field.dataType).lower().replace(\"type\", \"\"),\n",
    "            \"description\": f\"Column: {field.name}\",  # Update with actual description\n",
    "            \"isNullable\": field.nullable\n",
    "        }\n",
    "        columns.append(col_def)\n",
    "    \n",
    "    return columns\n",
    "\n",
    "\n",
    "def print_columns_as_python(columns: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Print column definitions as Python code for copy-paste.\n",
    "    \"\"\"\n",
    "    print(\"OUTPUT_COLUMNS = [\")\n",
    "    for col in columns:\n",
    "        print(\"    {\")\n",
    "        for key, value in col.items():\n",
    "            if isinstance(value, str):\n",
    "                print(f'        \"{key}\": \"{value}\",')\n",
    "            else:\n",
    "                print(f'        \"{key}\": {value},')\n",
    "        print(\"    },\")\n",
    "    print(\"]\")\n",
    "\n",
    "\n",
    "# Example usage (uncomment in Databricks):\n",
    "# df = spark.table(\"bronze.my_table\")\n",
    "# columns = extract_columns_from_dataframe(df)\n",
    "# print_columns_as_python(columns)\n",
    "\n",
    "print(\"DataFrame schema extraction functions loaded.\")\n",
    "print(\"To use: uncomment the example lines above in a Databricks notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Parse Existing Python Transformation\n",
    "\n",
    "Extract metadata from an existing Python transformation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "# HELPER: Parse Python transformation file\n",
    "# ════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "import ast\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def parse_transformation_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a Python transformation file and extract metadata.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to Python transformation file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with extracted metadata\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        source = f.read()\n",
    "    \n",
    "    tree = ast.parse(source)\n",
    "    \n",
    "    result = {\n",
    "        \"module_name\": Path(file_path).stem,\n",
    "        \"classes\": [],\n",
    "        \"functions\": [],\n",
    "        \"docstring\": ast.get_docstring(tree),\n",
    "        \"config_params\": []\n",
    "    }\n",
    "    \n",
    "    for node in ast.walk(tree):\n",
    "        # Find classes\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            class_info = {\n",
    "                \"name\": node.name,\n",
    "                \"docstring\": ast.get_docstring(node),\n",
    "                \"methods\": []\n",
    "            }\n",
    "            \n",
    "            for item in node.body:\n",
    "                if isinstance(item, ast.FunctionDef):\n",
    "                    class_info[\"methods\"].append(item.name)\n",
    "            \n",
    "            result[\"classes\"].append(class_info)\n",
    "        \n",
    "        # Find top-level functions\n",
    "        elif isinstance(node, ast.FunctionDef) and isinstance(node, ast.FunctionDef):\n",
    "            if not any(isinstance(parent, ast.ClassDef) for parent in ast.walk(tree)):\n",
    "                result[\"functions\"].append({\n",
    "                    \"name\": node.name,\n",
    "                    \"docstring\": ast.get_docstring(node)\n",
    "                })\n",
    "        \n",
    "        # Find kwargs.get() calls to extract config params\n",
    "        if isinstance(node, ast.Call):\n",
    "            if (isinstance(node.func, ast.Attribute) and \n",
    "                node.func.attr == 'get' and\n",
    "                isinstance(node.func.value, ast.Name) and\n",
    "                node.func.value.id == 'kwargs'):\n",
    "                if node.args:\n",
    "                    param_name = node.args[0]\n",
    "                    if isinstance(param_name, ast.Constant):\n",
    "                        param_info = {\"name\": param_name.value}\n",
    "                        if len(node.args) > 1:\n",
    "                            default = node.args[1]\n",
    "                            if isinstance(default, ast.Constant):\n",
    "                                param_info[\"default\"] = default.value\n",
    "                        result[\"config_params\"].append(param_info)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def print_transformation_info(info: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Print extracted transformation information.\n",
    "    \"\"\"\n",
    "    print(f\"Module: {info['module_name']}\")\n",
    "    print(f\"Docstring: {info['docstring'][:100] if info['docstring'] else 'None'}...\")\n",
    "    print()\n",
    "    \n",
    "    if info['classes']:\n",
    "        print(\"Classes:\")\n",
    "        for cls in info['classes']:\n",
    "            print(f\"  - {cls['name']}\")\n",
    "            if cls['methods']:\n",
    "                print(f\"    Methods: {', '.join(cls['methods'])}\")\n",
    "    \n",
    "    if info['config_params']:\n",
    "        print(\"\\nConfig Parameters (from kwargs.get()):\")\n",
    "        for param in info['config_params']:\n",
    "            default = param.get('default', 'N/A')\n",
    "            print(f\"  - {param['name']}: default={default}\")\n",
    "        \n",
    "        print(\"\\nSuggested TRANSFORMATION_CONFIG:\")\n",
    "        print(\"TRANSFORMATION_CONFIG = {\")\n",
    "        for param in info['config_params']:\n",
    "            default = param.get('default', 'None')\n",
    "            if isinstance(default, str):\n",
    "                print(f'    \"{param[\"name\"]}\": \"{default}\",')\n",
    "            else:\n",
    "                print(f'    \"{param[\"name\"]}\": {default},')\n",
    "        print(\"}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# info = parse_transformation_file(\"../nova_framework/transformations/python/product_sales_summary_example.py\")\n",
    "# print_transformation_info(info)\n",
    "\n",
    "print(\"Transformation file parser loaded.\")\n",
    "print(\"To use: uncomment the example lines above and provide your file path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
